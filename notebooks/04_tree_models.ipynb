{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c38d3fe-45af-4270-a3cc-28505b20a814",
   "metadata": {},
   "source": [
    "# 04 — Tree-Based Models\n",
    "\n",
    "Objective:\n",
    "Evaluate non-linear models to capture interactions and complex pricing behavior\n",
    "in used car valuation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6626393-d0f4-4890-bdc2-2411b5a1cbd0",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4736408-a040-4f17-a32b-21e6610fa7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ea327b-f303-4f7f-b642-a111309b5714",
   "metadata": {},
   "source": [
    "## 2. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fddfb64e-d9a1-4220-a08c-a41a3b70aec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96000, 14)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/processed/vehicles_feature_audited.csv\")\n",
    "df_small = df.sample(n=120000, random_state=42)\n",
    "\n",
    "X = df_small.drop(columns=[\"price\", \"log_price\"])\n",
    "y = df_small[\"log_price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9c6581-06aa-4629-a431-af3b03b0b2a5",
   "metadata": {},
   "source": [
    "## 3. Define Column Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f86a6e-b991-4d7b-9283-cac342b61b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = [\"year\", \"odometer\"]\n",
    "\n",
    "target_encode_cols = [\"model\", \"region\"]\n",
    "\n",
    "onehot_cols = [\n",
    "    \"manufacturer\", \"condition\", \"cylinders\", \"fuel\",\n",
    "    \"title_status\", \"transmission\", \"drive\",\n",
    "    \"size\", \"type\", \"paint_color\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ebff8e-6343-4d3d-a9b5-f87a719fd940",
   "metadata": {},
   "source": [
    "## 4. Tree Preprocessing\n",
    "\n",
    "Tree models do not require scaling.\n",
    "We retain:\n",
    "- Target encoding for high-cardinality features\n",
    "- OneHot encoding for low-cardinality features\n",
    "- Numerical features passed through unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b6cc9b-7faa-4e7c-98bf-98c07065a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"onehot\",\n",
    "         OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"),\n",
    "         onehot_cols),\n",
    "        \n",
    "        (\"target\",\n",
    "         ce.TargetEncoder(cols=target_encode_cols, smoothing=10),\n",
    "         target_encode_cols),\n",
    "        \n",
    "        (\"num\",\n",
    "         \"passthrough\",\n",
    "         numerical_cols)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab10e5f8-cccb-445a-89cf-e83d2c04e664",
   "metadata": {},
   "source": [
    "## 5. Random Forest (Lightweight Baseline)\n",
    "\n",
    "We use:\n",
    "- 50 trees\n",
    "- Max depth = 20\n",
    "- 3-fold CV\n",
    "To quickly evaluate whether non-linear modeling improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "972a25a3-f71f-421e-9502-1f573b1f62f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = Pipeline(steps=[\n",
    "    (\"preprocessing\", tree_preprocessor),\n",
    "    (\"regressor\", RandomForestRegressor(\n",
    "        n_estimators=50,\n",
    "        max_depth=20,\n",
    "        random_state=42,\n",
    "        n_jobs=4\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da2fccb7-6bfa-4cce-85df-616549eb949b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\py3.10-TF2.0\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest CV RMSE (log): 0.8349978398880267\n",
      "Per fold: [0.82671401 0.8319483  0.84633121]\n",
      "Std Dev: 0.008293907958951335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\py3.10-TF2.0\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Cross-Validation\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "rf_cv_rmse = -cross_val_score(\n",
    "    rf_model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=kf,\n",
    "    scoring=\"neg_root_mean_squared_error\"\n",
    ")\n",
    "\n",
    "print(\"Random Forest CV RMSE (log):\", rf_cv_rmse.mean())\n",
    "print(\"Per fold:\", rf_cv_rmse)\n",
    "print(\"Std Dev:\", rf_cv_rmse.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2142a328-2f82-43dd-b850-63ee34192730",
   "metadata": {},
   "source": [
    "## 6. LightGBM Baseline (Boosted Trees)\n",
    "\n",
    "We evaluate Gradient Boosting using LightGBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0818f54a-3412-4693-b033-31a6717f39ab",
   "metadata": {},
   "source": [
    "### Motivation for Gradient Boosting (LightGBM)\n",
    "\n",
    "Linear models underperformed (CV RMSE ≈ 0.995), indicating bias and inability to capture non-linear relationships.\n",
    "\n",
    "Random Forest significantly improved performance (CV RMSE ≈ 0.835), confirming:\n",
    "- Non-linear structure in pricing\n",
    "- Strong feature interactions\n",
    "- Tree-based models are more suitable\n",
    "\n",
    "However, Random Forest:\n",
    "- Averages independent trees\n",
    "- Reduces variance but does not aggressively reduce bias\n",
    "\n",
    "Gradient Boosting builds trees sequentially, where each new tree corrects previous residual errors. \n",
    "This typically leads to stronger performance on structured/tabular data.\n",
    "\n",
    "LightGBM is chosen because:\n",
    "- It is optimized for large datasets\n",
    "- Uses histogram-based splitting (faster training)\n",
    "- Handles high-dimensional features efficiently\n",
    "- Is widely used in industry for tabular ML problems\n",
    "\n",
    "Objective:\n",
    "Evaluate whether boosting further reduces log RMSE compared to Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "389f4148-158d-48b8-b822-abac00c82ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_model = Pipeline(steps=[\n",
    "    (\"preprocessing\", tree_preprocessor),\n",
    "    (\"regressor\", lgb.LGBMRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=-1,\n",
    "        random_state=42,\n",
    "        n_jobs=4,\n",
    "        verbosity=-1\n",
    "    ))\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92ef8897-5dcf-422f-84f9-594677b11544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM CV RMSE (log): 0.8277661239659446\n",
      "Per fold: [0.81881676 0.83256868 0.83191293]\n",
      "Std Dev: 0.006333814631087257\n"
     ]
    }
   ],
   "source": [
    "#Cross-Validation\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "lgb_cv_rmse = -cross_val_score(\n",
    "    lgb_model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=kf,\n",
    "    scoring=\"neg_root_mean_squared_error\"\n",
    ")\n",
    "\n",
    "print(\"LightGBM CV RMSE (log):\", lgb_cv_rmse.mean())\n",
    "print(\"Per fold:\", lgb_cv_rmse)\n",
    "print(\"Std Dev:\", lgb_cv_rmse.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec11814c-b0df-4d0e-a266-4bb0f09c813f",
   "metadata": {},
   "source": [
    "## 7. LightGBM Hyperparameter Tuning (Small Grid)\n",
    "\n",
    "Objective:\n",
    "Refine model complexity and learning dynamics to determine whether \n",
    "further bias reduction is possible beyond the baseline LightGBM model (CV RMSE ≈ 0.828).\n",
    "\n",
    "We tune only high-impact parameters:\n",
    "\n",
    "- num_leaves → controls tree complexity\n",
    "- learning_rate → controls step size of boosting\n",
    "- n_estimators → number of boosting rounds\n",
    "\n",
    "We use a small controlled grid to avoid overfitting and excessive compute.\n",
    "Tuning is performed on the 120k sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6e37edc-f118-46ea-9e4a-ec1cfbc8a7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaves=31, LR=0.1, Est=200 → RMSE=0.8278\n",
      "Leaves=31, LR=0.1, Est=400 → RMSE=0.8115\n",
      "Leaves=31, LR=0.05, Est=200 → RMSE=0.8421\n",
      "Leaves=31, LR=0.05, Est=400 → RMSE=0.8258\n",
      "Leaves=63, LR=0.1, Est=200 → RMSE=0.8129\n",
      "Leaves=63, LR=0.1, Est=400 → RMSE=0.7978\n",
      "Leaves=63, LR=0.05, Est=200 → RMSE=0.8253\n",
      "Leaves=63, LR=0.05, Est=400 → RMSE=0.8108\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "param_grid = {\n",
    "    \"num_leaves\": [31, 63],\n",
    "    \"learning_rate\": [0.1, 0.05],\n",
    "    \"n_estimators\": [200, 400]\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "results = []\n",
    "\n",
    "for num_leaves, lr, n_est in product(\n",
    "        param_grid[\"num_leaves\"],\n",
    "        param_grid[\"learning_rate\"],\n",
    "        param_grid[\"n_estimators\"]):\n",
    "\n",
    "    model = Pipeline(steps=[\n",
    "        (\"preprocessing\", tree_preprocessor),\n",
    "        (\"regressor\", lgb.LGBMRegressor(\n",
    "            num_leaves=num_leaves,\n",
    "            learning_rate=lr,\n",
    "            n_estimators=n_est,\n",
    "            max_depth=-1,\n",
    "            random_state=42,\n",
    "            n_jobs=4,\n",
    "            verbosity=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    cv_rmse = -cross_val_score(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=kf,\n",
    "        scoring=\"neg_root_mean_squared_error\"\n",
    "    )\n",
    "\n",
    "    mean_rmse = cv_rmse.mean()\n",
    "\n",
    "    print(f\"Leaves={num_leaves}, LR={lr}, Est={n_est} → RMSE={mean_rmse:.4f}\")\n",
    "\n",
    "    results.append((num_leaves, lr, n_est, mean_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5f28f1-d68b-4957-b8ca-decf858fd6a3",
   "metadata": {},
   "source": [
    "## 8. Overfitting Diagnostic (Train vs CV Error)\n",
    "\n",
    "We evaluate whether the best LightGBM configuration\n",
    "is overfitting by comparing training RMSE with cross-validation RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b02130f-78b3-4217-9cd0-7111b53ba9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = Pipeline(steps=[\n",
    "    (\"preprocessing\", tree_preprocessor),\n",
    "    (\"regressor\", lgb.LGBMRegressor(\n",
    "        num_leaves=63,\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=400,\n",
    "        max_depth=-1,\n",
    "        random_state=42,\n",
    "        n_jobs=4,\n",
    "        verbosity=-1\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17bc16c2-dc57-4180-87a3-a06b37056cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE (log): 0.5585516976445868\n",
      "CV RMSE (log): 0.7978\n",
      "Gap: 0.23924830235541317\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "train_preds = best_model.predict(X_train)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, train_preds))\n",
    "\n",
    "print(\"Train RMSE (log):\", train_rmse)\n",
    "print(\"CV RMSE (log): 0.7978\")\n",
    "print(\"Gap:\", 0.7978 - train_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dda0ad-c642-4a7a-a41f-c6e8007571ce",
   "metadata": {},
   "source": [
    "## 9. Regularized LightGBM (Variance Control)\n",
    "\n",
    "We add regularization to reduce overfitting observed in the tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b23f74c6-a4e2-4adb-99c3-48a1f964a904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized LightGBM CV RMSE (log): 0.8045971295183284\n",
      "Std Dev: 0.005204331268911649\n"
     ]
    }
   ],
   "source": [
    "regularized_model = Pipeline(steps=[\n",
    "    (\"preprocessing\", tree_preprocessor),\n",
    "    (\"regressor\", lgb.LGBMRegressor(\n",
    "        num_leaves=63,\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=400,\n",
    "        min_child_samples=50,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=4,\n",
    "        verbosity=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "reg_cv_rmse = -cross_val_score(\n",
    "    regularized_model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=kf,\n",
    "    scoring=\"neg_root_mean_squared_error\"\n",
    ")\n",
    "\n",
    "print(\"Regularized LightGBM CV RMSE (log):\", reg_cv_rmse.mean())\n",
    "print(\"Std Dev:\", reg_cv_rmse.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d1c61-bbfd-473c-8b18-7303cd7140ec",
   "metadata": {},
   "source": [
    "## 10. Model Selection Decision\n",
    "\n",
    "We compared multiple LightGBM configurations on the 120k sample dataset.\n",
    "\n",
    "Best configuration:\n",
    "\n",
    "- num_leaves = 63\n",
    "- learning_rate = 0.1\n",
    "- n_estimators = 400\n",
    "\n",
    "Cross-Validation RMSE (log) ≈ 0.798\n",
    "\n",
    "Observations:\n",
    "- Increasing model capacity reduced bias and improved validation performance.\n",
    "- Strong regularization slightly worsened validation RMSE.\n",
    "- Although the model shows a noticeable train–validation gap, \n",
    "  validation performance remains strongest for this configuration.\n",
    "\n",
    "Decision:\n",
    "Select this LightGBM configuration as the final candidate model.\n",
    "\n",
    "Next Step:\n",
    "Train this configuration on the full dataset and evaluate once on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f444fb2-43f2-431d-8a35-e2fabbee65b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3.10-TF2.0]",
   "language": "python",
   "name": "conda-env-py3.10-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
